{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMwDtXuYZZMbTdkR2cHqCa5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia"
    },
    "language_info": {
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gauthameshwar/RAQS_QuantumInspiredAlgos/blob/master/notebooks/RSVDnDataCompression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomized SVD → Tensor Networks for Data Compression\n",
        "\n",
        "**Notebook author:** Gauthameshwar Sundaravadivel\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> **You’ll learn**\n",
        "> - **Randomized SVD (RSVD)**: faster than classical SVD; power/subspace iterations for accuracy.\n",
        "> - **Tensor-Train (TT/MPS)**: compress multiway data with small ranks.\n",
        "> - **Case study**: gene-expression compression with TT.\n",
        ">\n",
        "> **Prerequisites:** Linear algebra, basic Julia.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "  <summary>Outline (click to expand)</summary>\n",
        "\n",
        "  1. SVD recap → low-rank ≈ compression  \n",
        "  2. RSVD algorithm, complexity & error, power iterations  \n",
        "  3. Tensor networks (MPS/TT) without physics baggage  \n",
        "  4. TT-SVD (Oseledets) and where RSVD slots in  \n",
        "  5. Gene-expression compression mini-project  \n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Prerequisites:** Linear algebra, basic Julia."
      ],
      "metadata": {
        "id": "Gi0nIj0rwCRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Setup\n",
        "\n",
        "We borrow the package deps for this notebook from the home GitHub repo of this notebook, and activate it before diving into the exercises."
      ],
      "metadata": {
        "id": "0tl9f-hv9405"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5LSPpy4j8M_",
        "outputId": "93601f02-0c1a-42a8-8cdc-79cdedcd3ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `/content/RAQS_QuantumInspiredAlgos/env`\n"
          ]
        }
      ],
      "source": [
        "const USER = \"Gauthameshwar\"\n",
        "const REPO = \"RAQS_QuantumInspiredAlgos\"\n",
        "const BRANCH = \"master\"\n",
        "\n",
        "cd(\"/content\")\n",
        "\n",
        "if !isdir(REPO)\n",
        "  run(`git clone --depth 1 -b $BRANCH https://github.com/$USER/$REPO.git`)\n",
        "end\n",
        "\n",
        "import Pkg\n",
        "proj = joinpath(\"/content\", REPO, \"env\")  # where Project.toml lives\n",
        "isfile(joinpath(proj, \"Project.toml\")) || error(\"Project.toml not found: $proj\")\n",
        "Pkg.activate(proj)\n",
        "Pkg.instantiate()   # uses Manifest.toml if present; pins exact versions\n",
        "\n",
        "# proj = joinpath(pwd(), \"notebooks\", \"env\") # e.g., .../notebooks/env/Project.toml\n",
        "# isdir(proj) || error(\"Project path not found: $proj\") Pkg.activate(proj)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using LinearAlgebra, Plots, Random, Logging"
      ],
      "metadata": {
        "id": "q5aWIGwT1ac4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Randomised SVD\n",
        "\n",
        "---\n",
        "> **Motivation**\n",
        ">\n",
        "> Many real datasets(images, audio, experimental measurements) can often be described well with only a **few important directions (singular vectors)**. That means the matrix is **approximately low rank**. A full SVD finds _all_ directions and can be slow on big matrices. RSVD focuses on the important part first, so it's **much faster** when you only need the top $k$ components\n",
        "\n",
        "---\n",
        "\n",
        "> **Idea in a nutshell**\n",
        ">\n",
        "> Instead of SVD-ing a big matrix $A$, we first find a small **orthonormal basis $Q$** that spans (most of) the columns of $A$. Then we SVD a **small matrix $B=Q^TA$** and lift the result back with $Q$.\n",
        "\n",
        "---\n",
        "\n",
        "Quick notation and definitions:\n",
        "\n",
        "- Matrix $A\\in \\mathbb{R}^{m\\times n}$: A grid of numbers with $m$ rows and $n$ columns.\n",
        "- Rank $k$: How many important directions (singular vectors) we wish to keep.\n",
        "- Orthonormal basis $Q\\in \\mathbb{R}^{m\\times l}$: Columns of $Q$ are unit-length and mutually perpendicular.\n",
        "- SVD: Factorisation of the matrix into $A = U \\Sigma V^T$ such that $U \\in \\mathbb{R}^{m\\times m}$ and $V \\in \\mathbb{R}^{n\\times n}$ are orthonormal matrices, and $\\Sigma \\in \\mathbb{R}^{m\\times n}$ is a diagonal matrix with the singular values as its diagonal entries (from largest to smallest in magnitude).\n",
        "- Oversampling parameter $p$: Additional number of rank into the sampling so we don't miss important information in the last singular vectors (optimal values around 5-20).\n",
        "- Power iterations $q$: Extra passes of the subspace so we sharpen the accuracy of the singular values and its vectors.\n",
        "\n",
        "\n",
        "In a standard SVD, given a matrix $A\\in \\mathbb{R}^{m\\times n}$, we compute\n",
        "$$\n",
        "A = U \\Sigma V^T.\n",
        "$$\n",
        "The best algorithm of SVD in Julia is provided by `LinearAlgebra`'s `LAPACK` support which has the runtime complexity of\n",
        "$$\n",
        "O(mn\\times \\text{min}(m, n)).\n",
        "$$\n",
        "This can get slow when both $m$ and $n$ are large. In the worst case scenario, where we have a large square matrix, the complexity scales as $O(n^3)$.\n",
        "\n",
        "Also, the memory complexity of performing the SVD goes as\n",
        "$$O(mn + m^2 + n^2),$$ which is quadratic in the largest size."
      ],
      "metadata": {
        "id": "a_hRKKwOGyqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The RSVD algorithm\n",
        "\n",
        "> Goal: To get a good rank‑$k$ approximation quickly by working with a randomised subspace.\n",
        "\n",
        "Stage A — Find a good basis\n",
        "\n",
        "1. Make a random test matrix  with $\\Omega \\in \\mathbb{R}^{n\\times l}$ with $l = k + p$.\n",
        "\n",
        "2. Form a sample of ’s column space: $Y = A\\Omega \\in \\mathbb{R}^{m\\times l}$.\n",
        "\n",
        "3. Compute a thin QR of $Y$: $Y = QR$. Keep $Q \\in \\mathbb{R}^{m\\times l}$.\n",
        "\n",
        "Stage B — Work in small coordinates\n",
        "\n",
        "4. Compress $A$ to a small matrix: $B = Q^TA \\in \\mathbb{R}^{l\\times n}$.\n",
        "\n",
        "5. Do SVD on the small matrix: $B = \\tilde{U}\\Sigma V^T$.\n",
        "\n",
        "6. Map back: $U \\approx Q \\tilde{U}$.\n",
        "\n",
        "7. Truncate to rank $k$: keep the first $k$ columns of $U$ and $V$, and the first $k$ values in $\\Sigma$.\n",
        "\n",
        "Optional accuracy boost (power iterations): if the singular values don’t drop quickly, repeat extra multiplies $q$ times so the important directions stand out more.\n"
      ],
      "metadata": {
        "id": "MlAJ8SUWWy7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function rsvd(A::AbstractMatrix{<:Union{Real, Complex}}, l::Int, p::Int=10)\n",
        "    m, n = size(A)\n",
        "    @info \"Starting Randomized SVD\" DataSize=(m, n) RequestedSingularValues=l OversamplingParameter=p TotalColumnsForSampling=(l+p)\n",
        "\n",
        "    # sample a random matrix Y in the subspace of A\n",
        "    @info \"1. Sampling random matrix Y...\"\n",
        "    time_Y = @elapsed Y = A * randn(n, l + p)\n",
        "    @info \"Y sampling completed\" Time=time_Y\n",
        "\n",
        "    # form the sample matrix B\n",
        "    @info \"2. Performing QR factorization of Y and forming sample matrix B...\"\n",
        "    time_B = @elapsed Q = qr(Y).Q[:, 1:min(l+p, size(qr(Y).Q, 2))]\n",
        "    B = Q' * A\n",
        "    @info \"B formation completed\" Time=time_B\n",
        "\n",
        "    # compute the SVD of B\n",
        "    @info \"3. Computing the SVD of B...\"\n",
        "    time_svd_B = @elapsed Ũ, D, V = svd(B)\n",
        "    @info \"SVD of B completed\" Time=time_svd_B\n",
        "\n",
        "    # compute the approximate singular vectors of A\n",
        "    @info \"4. Computing the approximate singular vectors of A...\"\n",
        "    time_U = @elapsed U = Q * Ũ\n",
        "    @info \"Approximate U computation completed\" Time=time_U\n",
        "\n",
        "    @info \"Randomized SVD completed\"\n",
        "\n",
        "    return U, D, V\n",
        "end\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2zAjxQ5CWt9",
        "outputId": "ea54373c-ab0b-4240-dc56-199ddd431593"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rsvd (generic function with 2 methods)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runtime and memory complexity of RSVD\n",
        "1. Runtime\n",
        "- The heavy work in this algorithm is done in the largest matrix multiplications: $Y = A \\Omega$. When the matrix is dense (worst-case scenario), it has a rough cost of $O(mnl)$.\n",
        "\n",
        "- The QR and SVD has $O(ml^2 + nl^2)$ together. Thus, they are not the bottleneck of this algorithm for small values of $l$.\n",
        "\n",
        "2. Memory\n",
        "- Excluding the storing of the original data $A$, the memory complexity of RSVD is $O((m + n)l)$.\n",
        "- This makes it more memory efficient for larger data sizes than SVD since it scales linearly in the largest dimension."
      ],
      "metadata": {
        "id": "gFFvztGibRuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a random matrix to test the RSVD\n",
        "m, n = 10000, 10000\n",
        "A = randn(m, n)\n",
        "\n",
        "rsvd(A, 20);\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yx7kCyXEold",
        "outputId": "e8062094-5860-43aa-fb99-05c668d5e2db"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mStarting Randomized SVD\n",
            "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  DataSize = (10000, 10000)\n",
            "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  RequestedSingularValues = 20\n",
            "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m  OversamplingParameter = 10\n",
            "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  TotalColumnsForSampling = 30\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m1. Sampling random matrix Y...\n",
            "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mY sampling completed\n",
            "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  Time = 0.404582439\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m2. Performing QR factorization of Y and forming sample matrix B...\n",
            "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mB formation completed\n",
            "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  Time = 0.01548268\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m3. Computing the SVD of B...\n",
            "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSVD of B completed\n",
            "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  Time = 0.024392761\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m4. Computing the approximate singular vectors of A...\n",
            "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mApproximate U computation completed\n",
            "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  Time = 0.00158533\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mRandomized SVD completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy of RSVD"
      ],
      "metadata": {
        "id": "vTpyIG3mTAVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ktgz3xk_a5m8"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}